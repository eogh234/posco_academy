{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공 신경망_분류\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#파일 불러오기\n",
    "df_raw = pd.read_csv(\"../data/유방암.csv\", engine='python', encoding='cp949')\n",
    "df_raw.head()\n",
    "\n",
    "df_raw.isnull().sum(axis = 0) #결측치 확인 및 처리\n",
    "\n",
    "# diagnosis 변수 \"양성\" or \"음성\"을 1과 0으로 바꾼다.\n",
    "df_raw[\"diagnosis\"] = np.where(df_raw[\"diagnosis\"] == \"양성\", 0, 1)\n",
    "df_raw.head()\n",
    "\n",
    "# 목표 변수 diagnosis를 y로 설정하고 나머지 변수들을 x로 나눈다.\n",
    "df_raw_y = df_raw[\"diagnosis\"]\n",
    "df_raw_x = df_raw.drop(\"diagnosis\", axis = 1, inplace = False)\n",
    "df_train_x, df_test_x, df_train_y, df_test_y = train_test_split(df_raw_x, df_raw_y, test_size = 0.3, random_state = 1234)\n",
    "\n",
    "nn_uncustomized = MLPClassifier(random_state=1234)\n",
    "nn_uncustomized.fit(df_train_x, df_train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(nn_uncustomized.score(df_train_x, df_train_y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(nn_uncustomized.score(df_test_x, df_test_y)))\n",
    "\n",
    "## 임의로 데이터를 트레이닝 6 : 테스트 4으로 나누었을 때,\n",
    "## 트레이닝 데이터셋의 정확도는 92.4%\n",
    "## 테스트 데이터셋의 정확도는 89.6%로 측정된다.\n",
    "\n",
    "nn_uncustomized\n",
    "\n",
    "# 은닉층(Hidden Layer)\n",
    "## 은닉층 개수와 층별 노드 개수를 지정하여 은닉층의 복잡도를 결정한다.\n",
    "## => 은닉층이 복잡할수록 모델의 과대적합 경향이 나타난다.\n",
    "## Hidden Layer(은닉층) 변경\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_hidden = [20 * hidden for hidden in range(1, 9)]\n",
    "\n",
    "for v_hidden in para_hidden:\n",
    "    nn = MLPClassifier(hidden_layer_sizes=v_hidden, random_state=1234)\n",
    "    nn.fit(df_train_x, df_train_y)\n",
    "    train_accuracy.append(nn.score(df_train_x, df_train_y))\n",
    "    test_accuracy.append(nn.score(df_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_hidden = pd.DataFrame()\n",
    "df_accuracy_hidden[\"HiddenLayer\"] = para_hidden\n",
    "df_accuracy_hidden[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_hidden[\"TestAccuracy\"] = test_accuracy\n",
    "\n",
    "df_accuracy_hidden.round(3)\n",
    "\n",
    "## HiddenLayer가 100일 때 최대의 정확도가 측정된다.\n",
    "\n",
    "plt.plot(para_hidden, train_accuracy, linestyle = \"-\", label = \"Train Accuracy\")\n",
    "plt.plot(para_hidden, test_accuracy, linestyle = \"--\", label = \"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Hidden Layer\")\n",
    "plt.legend()\n",
    "\n",
    "## Activation(활성화 함수) 변경\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_function = [\"logistic\", \"tanh\", \"relu\"]\n",
    "\n",
    "for v_function in para_function:\n",
    "    nn = MLPClassifier(activation=v_function, random_state=1234)\n",
    "    nn.fit(df_train_x, df_train_y)\n",
    "    train_accuracy.append(nn.score(df_train_x, df_train_y))\n",
    "    test_accuracy.append(nn.score(df_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_function = pd.DataFrame()\n",
    "df_accuracy_function[\"ActivationFunction\"] = para_function\n",
    "df_accuracy_function[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_function[\"TestAccuracy\"] = test_accuracy\n",
    "\n",
    "df_accuracy_function.round(3)\n",
    "\n",
    "# 활성화 함수(Activation Function)\n",
    "\n",
    "## 입력층 또는 이전의 은닉층의 정보를 다음 층으로의 출려값을 결정하는 함수\n",
    "## 3가지의 활성화 함수를 적용했을 때, tanh 함수가 가장 정확도가 높았다.\n",
    "\n",
    "plt.plot(para_function, train_accuracy, linestyle = \"-\", label = \"Train Accuracy\")\n",
    "plt.plot(para_function, test_accuracy, linestyle = \"--\", label = \"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Activation Function\")\n",
    "plt.legend()\n",
    "\n",
    "v_feature_name = df_train_x.columns\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_raw_x)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=v_feature_name)\n",
    "df_scaled.head()\n",
    "\n",
    "## 설명변수의 구간을 동일하게 변경(스케일링)한다.\n",
    "## => 트레이닝 셋과 테스트 셋의 사이즈를 맞추어 준다.\n",
    "\n",
    "df_scaled_train_x, df_scaled_test_x = train_test_split(df_scaled, test_size = 0.3, random_state = 1234)\n",
    "print(\"train data X size : {}\".format(df_scaled_train_x.shape))\n",
    "print(\"test data X size : {}\".format(df_scaled_test_x.shape))\n",
    "\n",
    "nn_scaled = MLPClassifier(random_state=1234)\n",
    "nn_scaled.fit(df_scaled_train_x, df_train_y)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(nn_scaled.score(df_scaled_train_x, df_train_y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(nn_scaled.score(df_scaled_test_x, df_test_y)))\n",
    "\n",
    "## 스케일링 이후에 정확도를 측정하면 이전의 정확도에 비해\n",
    "## 증가한 정확도를 얻을 수 있다.\n",
    "\n",
    "## 스케일링 이후에 은닉층을 다시 변경하여 적용한다.\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_hidden = [20 * hidden for hidden in range(1, 11)]\n",
    "\n",
    "for v_hidden in para_hidden:\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(v_hidden, v_hidden), random_state=1234)\n",
    "    nn.fit(df_scaled_train_x, df_train_y)\n",
    "    train_accuracy.append(nn.score(df_scaled_train_x, df_train_y))\n",
    "    test_accuracy.append(nn.score(df_scaled_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_hidden = pd.DataFrame()\n",
    "df_accuracy_hidden[\"HiddenLayer\"] = para_hidden\n",
    "df_accuracy_hidden[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_hidden[\"TestAccuracy\"] = test_accuracy\n",
    "\n",
    "df_accuracy_hidden.round(3)\n",
    "\n",
    "plt.plot(para_hidden, train_accuracy, linestyle = \"-\", label = \"Train Accuracy\")\n",
    "plt.plot(para_hidden, test_accuracy, linestyle = \"--\", label = \"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Hidden Layer\")\n",
    "plt.legend()\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_function = [\"logistic\", \"tanh\", \"relu\"]\n",
    "\n",
    "for v_function in para_function:\n",
    "    nn = MLPClassifier(activation=v_function, hidden_layer_sizes = (120,120), random_state=1234)\n",
    "    nn.fit(df_train_x, df_train_y)\n",
    "    train_accuracy.append(nn.score(df_scaled_train_x, df_train_y))\n",
    "    test_accuracy.append(nn.score(df_scaled_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_function = pd.DataFrame()\n",
    "df_accuracy_function[\"ActivationFunction\"] = para_function\n",
    "df_accuracy_function[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_function[\"TestAccuracy\"] = test_accuracy\n",
    "\n",
    "df_accuracy_function.round(3)\n",
    "\n",
    "## relu 활성화 함수의 정확도가 가장 높게 나왔다.\n",
    "\n",
    "plt.plot(para_function, train_accuracy, linestyle = \"-\", label = \"Train Accuracy\")\n",
    "plt.plot(para_function, test_accuracy, linestyle = \"--\", label = \"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Activation Function\")   \n",
    "plt.legend()\n",
    "\n",
    "# 가중치 최적화\n",
    "## 신경망 모델의 학습 결과에 따른  손실함수 값을 최소화하는 하이퍼 파라미터 탐색 및 최적화\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_solver = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "\n",
    "for v_solver in para_solver:\n",
    "    nn = MLPClassifier(solver = v_solver, activation=\"relu\", hidden_layer_sizes = (120,120), random_state=1234)\n",
    "    nn.fit(df_train_x, df_train_y)\n",
    "    train_accuracy.append(nn.score(df_scaled_train_x, df_train_y))\n",
    "    test_accuracy.append(nn.score(df_scaled_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_solver = pd.DataFrame()\n",
    "df_accuracy_solver[\"Solver\"] = para_solver\n",
    "df_accuracy_solver[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_solver[\"TestAccuracy\"] = test_accuracy\n",
    "\n",
    "df_accuracy_solver.round(3)\n",
    "\n",
    "## => \"adam\" 모델이 가장 정확도가 높은 것을 확인했다.\n",
    "\n",
    "plt.plot(para_solver, train_accuracy, linestyle = \"-\", label = \"Train Accuracy\")\n",
    "plt.plot(para_solver, test_accuracy, linestyle = \"--\", label = \"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Solver\")\n",
    "plt.legend()\n",
    "\n",
    "# 미니배치\n",
    "## 전체 학습 데이터를 배치 크기로 등분하여 각 배치 데이터를 순차적으로 실행하는 방법\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_batch = [20 * batch for batch in range(1, 10)]\n",
    "\n",
    "for v_batch in para_batch:\n",
    "    nn = MLPClassifier(batch_size=v_batch, solver = \"adam\", activation=\"relu\", hidden_layer_sizes = (120,120), random_state=1234)\n",
    "    nn.fit(df_train_x, df_train_y)\n",
    "    train_accuracy.append(nn.score(df_scaled_train_x, df_train_y))\n",
    "    test_accuracy.append(nn.score(df_scaled_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_batch = pd.DataFrame()\n",
    "df_accuracy_batch[\"Mini Batch\"] = para_batch\n",
    "df_accuracy_batch[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_batch[\"TestAccuracy\"] = test_accuracy\n",
    "\n",
    "df_accuracy_batch.round(3)\n",
    "\n",
    "## => mini batch의 크기를 20으로 했을 때 가장 높은 정확도를 기록했다.\n",
    "\n",
    "plt.plot(para_batch, train_accuracy, linestyle = \"-\", label = \"Train Accuracy\")\n",
    "plt.plot(para_batch, test_accuracy, linestyle = \"--\", label = \"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Mini Batch Size\")\n",
    "plt.legend()\n",
    "\n",
    "df_accuracy_batch.round(3)\n",
    "\n",
    "plt.plot(para_batch, train_accuracy, linestyle = \"-\", label = \"Train Accuracy\")\n",
    "plt.plot(para_batch, test_accuracy, linestyle = \"--\", label = \"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"Mini Batch Size\")\n",
    "plt.legend()\n",
    "\n",
    "nn_final = MLPClassifier(hidden_layer_sizes=(120, 120), activation=\"relu\", solver=\"adam\",\n",
    "                        batch_size=20, random_state= 1234)\n",
    "nn_final.fit(df_scaled_train_x, df_train_y)\n",
    "y_pred = nn_final.predict(df_scaled_test_x)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(nn_final.score(df_scaled_train_x, df_train_y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(nn_final.score(df_scaled_test_x, df_test_y)))\n",
    "print(\"Confusion matrix: \\n{}\".format(confusion_matrix(df_test_y, y_pred)))\n",
    "\n",
    "# 결론 : 트레이닝 데이터 정확도 : 100.0%\n",
    "# 테스트 데이터 정확도 : 97.9%\n",
    "# 정분류율 = (24 + 70) / (24 + 70 + 0 + 2) = 94.79%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'species'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'species'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-775b91ad918a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 목표 변수 diagnosis를 y로 설정하고 나머지 변수들을 x로 나눈다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf_raw_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"species\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mdf_raw_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"species\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdf_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'species'"
     ]
    }
   ],
   "source": [
    "# KNN(K-Nearest Neighbors) 분석\n",
    "## 거리 기반으로 이웃을 결정하며 새로운 사건이 발생했을 때 가장 근접한 k-이웃의 값을 평균해서 예측하거나 빈도가 많은 클래스로 분류하는 탐색적 기법\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#파일 불러오기\n",
    "df_raw = pd.read_csv(\"../data/IRIS.csv\", engine='python', encoding='cp949')\n",
    "df_raw.head()\n",
    "\n",
    "df_raw.isnull().sum(axis = 0) #결측치 확인 및 처리\n",
    "\n",
    "# diagnosis 변수 \"양성\" or \"음성\"을 1과 0으로 바꾼다.\n",
    "df_raw[\"c\"] = np.where(df_raw[\"SPECIES\"] == \"setosa\", 0, 1)\n",
    "df_raw[\"SPECIE\"] = np.where(df_raw[\"SPECIES\"] == \"versicolor\", 1, 2)\n",
    "df_raw.head()\n",
    "\n",
    "# 목표 변수 diagnosis를 y로 설정하고 나머지 변수들을 x로 나눈다.\n",
    "df_raw_y = df_raw[\"species\"]\n",
    "df_raw_x = df_raw.drop(\"species\", axis = 1, inplace = False)\n",
    "df_train_x, df_test_x, df_train_y, df_test_y = train_test_split(df_raw_x, df_raw_y, test_size = 0.3, random_state = 1234)\n",
    "\n",
    "# default 값으로 분류한 결과\n",
    "## 트레이닝 데이터 정확도 : 92.2%\n",
    "## 테스트 데이터 정확도 : 90.6%\n",
    "\n",
    "knn_uncustomized = KNeighborsClassifier()\n",
    "knn_uncustomized.fit(df_train_x, df_train_y)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(nn_uncustomized.score(df_train_x, df_train_y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(nn_uncustomized.score(df_test_x, df_test_y)))\n",
    "\n",
    "knn_uncustomized\n",
    "\n",
    "# n_neighbors : 이웃 수\n",
    "## 이웃 수가 작으면 과대적합의 위험이 증가한다.\n",
    "## 이웃 수가 크면 과소적합될 가능성이 증가한다.\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_n_neighbors = [i for i in range(3, 6)]\n",
    "\n",
    "for v_n_neighbors in para_n_neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = v_n_neighbors)\n",
    "    knn.fit(df_train_x, df_train_y)\n",
    "    train_accuracy.append(knn.score(df_train_x, df_train_y))\n",
    "    test_accuracy.append(knn.score(df_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_neighbors = pd.DataFrame()\n",
    "df_accuracy_neighbors[\"Neighbors\"] = para_n_neighbors\n",
    "df_accuracy_neighbors[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_neighbors[\"TestAccuracy\"] = test_accuracy\n",
    "df_accuracy_neighbors.round(3)\n",
    "\n",
    "## 가장 높은 정확도를 기록한 neighbors 값은 7이다.\n",
    "\n",
    "plt.plot(para_n_neighbors, train_accuracy, linestyle = \"-\", label = \"Train Accuracy\")\n",
    "plt.plot(para_n_neighbors, test_accuracy, linestyle = \"--\", label = \"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()\n",
    "\n",
    "# Weights : 예측에 사용되는 가중 함수\n",
    "## uniform : 균일한 가중치\n",
    "## disance : 거리의 역수로 가중치 부여, 가까운 이웃들 간에는 큰 영향값을 가진다.\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_n_neighbors = [i for i in range(3, 31)] * 2 \n",
    "para_weights = ([\"uniform\"] * 28) + ([\"distance\"] * 28)\n",
    "\n",
    "for (v_n_neighbors, v_weights) in zip(para_n_neighbors, para_weights):\n",
    "    knn = KNeighborsClassifier(n_neighbors = v_n_neighbors, weights=v_weights)\n",
    "    knn.fit(df_train_x, df_train_y)\n",
    "    train_accuracy.append(knn.score(df_train_x, df_train_y))\n",
    "    test_accuracy.append(knn.score(df_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_weights = pd.DataFrame()\n",
    "df_accuracy_weights[\"Neighbors\"] = para_n_neighbors\n",
    "df_accuracy_weights[\"Weights\"] = para_weights\n",
    "df_accuracy_weights[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_weights[\"TestAccuracy\"] = test_accuracy\n",
    "df_accuracy_weights.round(3)\n",
    "\n",
    "## uniform 일 때는 이웃 수가 7일 때 정확도가 가장 높다\n",
    "## distance 일 때는 6과 10일 때 가장 높다는 것을 확인할 수 있다.\n",
    "\n",
    "df_accuracy_weights_pivot = df_accuracy_weights.pivot(index = \"Neighbors\", columns = \"Weights\",\n",
    "                                                     values=[\"TrainAccuracy\", \"TestAccuracy\"])\n",
    "df_accuracy_weights_pivot\n",
    "\n",
    "level0 = df_accuracy_weights_pivot.columns.get_level_values(0)\n",
    "level1 = df_accuracy_weights_pivot.columns.get_level_values(1)\n",
    "df_accuracy_weights_pivot.columns = level0 + \"_\" + level1\n",
    "df_accuracy_weights_pivot.head()\n",
    "\n",
    "sns.lineplot(data = df_accuracy_weights_pivot)\n",
    "\n",
    "# 거리 계산 방법별 이웃 수 증가에 따른 정확도 변화\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_n_neighbors = [i for i in range(3, 31)] * 3\n",
    "para_metric = [\"minkowski\"] * 28 + [\"euclidean\"] * 28 + [\"manhattan\"] * 28\n",
    "\n",
    "for (v_n_neighbors, v_metric) in zip(para_n_neighbors, para_metric):\n",
    "    knn = KNeighborsClassifier(n_neighbors = v_n_neighbors, weights=\"uniform\", metric=v_metric)\n",
    "    knn.fit(df_train_x, df_train_y)\n",
    "    train_accuracy.append(knn.score(df_train_x, df_train_y))\n",
    "    test_accuracy.append(knn.score(df_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_metric = pd.DataFrame()\n",
    "df_accuracy_metric[\"Neighbors\"] = para_n_neighbors\n",
    "df_accuracy_metric[\"Metric\"] = para_metric\n",
    "df_accuracy_metric[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_metric[\"TestAccuracy\"] = test_accuracy\n",
    "df_accuracy_metric.groupby(\"Metric\").max().round(3)\n",
    "\n",
    "## 거리 계산 방법 3가지의 최대 정확도 값을 보았을 때\n",
    "## 모두 같은 값을  갖는다.\n",
    "## 트레이닝 정확도를 보았을 때 manhattan 방법이 가장 높은 정확도를 가지기 때문에 manhattan 방법을 채택했다.\n",
    "\n",
    "df_accuracy_metric_pivot = df_accuracy_metric.pivot(index = \"Neighbors\", columns = \"Metric\",\n",
    "                                                     values=[\"TrainAccuracy\", \"TestAccuracy\"])\n",
    "level0 = df_accuracy_metric_pivot.columns.get_level_values(0)\n",
    "level1 = df_accuracy_metric_pivot.columns.get_level_values(1)\n",
    "df_accuracy_metric_pivot.columns = level0 + \"_\" + level1\n",
    "sns.lineplot(data = df_accuracy_metric_pivot)\n",
    "\n",
    "## Scale 변경\n",
    "\n",
    "v_feature_name = df_train_x.columns\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_raw_x)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=v_feature_name)\n",
    "df_scaled.head()\n",
    "\n",
    "df_scaled_train_x, df_scaled_test_x = train_test_split(df_scaled, test_size = 0.4, random_state =1234)\n",
    "print(\"train data X size : {}\".format(df_scaled_train_x.shape))\n",
    "print(\"test data X size : {}\".format(df_scaled_test_x.shape))\n",
    "\n",
    "knn_scaled = KNeighborsClassifier()\n",
    "knn_scaled.fit(df_scaled_train_x, df_train_y)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(knn_scaled.score(df_scaled_train_x, df_train_y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(knn_scaled.score(df_scaled_test_x, df_test_y)))\n",
    "\n",
    "## 스케일링을 한 이후에 정확도는\n",
    "## 트레이닝 95.8%, 테스트 92.2%이다.\n",
    "\n",
    "## 이전의 92.2% 90.6%보다 향상되었다는 것을 알 수 있다.\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_n_neighbors = [i for i in range(3, 31)]\n",
    "\n",
    "for v_n_neighbors in para_n_neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = v_n_neighbors)\n",
    "    knn.fit(df_scaled_train_x, df_train_y)\n",
    "    train_accuracy.append(knn.score(df_scaled_train_x, df_train_y))\n",
    "    test_accuracy.append(knn.score(df_scaled_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_neighbors = pd.DataFrame()\n",
    "df_accuracy_neighbors[\"Neighbors\"] = para_n_neighbors\n",
    "df_accuracy_neighbors[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_neighbors[\"TestAccuracy\"] = test_accuracy\n",
    "df_accuracy_neighbors.round(3)\n",
    "\n",
    "## neighbors의 값을 3부터 30까지 변경하면서 정확도를 비교했을 때,\n",
    "## n_neighbors가 7일 때 test 정확도가 가장 높다는 것을 확인했다.\n",
    "\n",
    "plt.plot(para_n_neighbors, train_accuracy, linestyle = \"-\", label = \"Train Accuracy\")\n",
    "plt.plot(para_n_neighbors, test_accuracy, linestyle = \"--\", label = \"Test Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_n_neighbors = [i for i in range(3, 31)] * 2 \n",
    "para_weights = ([\"uniform\"] * 28) + ([\"distance\"] * 28)\n",
    "\n",
    "for (v_n_neighbors, v_weights) in zip(para_n_neighbors, para_weights):\n",
    "    knn = KNeighborsClassifier(n_neighbors = v_n_neighbors, weights=v_weights)\n",
    "    knn.fit(df_scaled_train_x, df_train_y)\n",
    "    train_accuracy.append(knn.score(df_scaled_train_x, df_train_y))\n",
    "    test_accuracy.append(knn.score(df_scaled_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_weights = pd.DataFrame()\n",
    "df_accuracy_weights[\"Neighbors\"] = para_n_neighbors\n",
    "df_accuracy_weights[\"Weights\"] = para_weights\n",
    "df_accuracy_weights[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_weights[\"TestAccuracy\"] = test_accuracy\n",
    "df_accuracy_weights.round(3)\n",
    "\n",
    "df_accuracy_weights_pivot = df_accuracy_weights.pivot(index = \"Neighbors\", columns = \"Weights\",\n",
    "                                                     values=[\"TrainAccuracy\", \"TestAccuracy\"])\n",
    "df_accuracy_weights_pivot\n",
    "\n",
    "level0 = df_accuracy_weights_pivot.columns.get_level_values(0)\n",
    "level1 = df_accuracy_weights_pivot.columns.get_level_values(1)\n",
    "df_accuracy_weights_pivot.columns = level0 + \"_\" + level1\n",
    "df_accuracy_weights_pivot.head()\n",
    "\n",
    "sns.lineplot(data = df_accuracy_weights_pivot)\n",
    "\n",
    "# 거리 계산 방법별 정확도\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "para_n_neighbors = [i for i in range(3, 31)] * 3\n",
    "para_metric = [\"minkowski\"] * 28 + [\"euclidean\"] * 28 + [\"manhattan\"] * 28\n",
    "\n",
    "for (v_n_neighbors, v_metric) in zip(para_n_neighbors, para_metric):\n",
    "    knn = KNeighborsClassifier(n_neighbors = v_n_neighbors, weights=\"uniform\", metric=v_metric)\n",
    "    knn.fit(df_scaled_train_x, df_train_y)\n",
    "    train_accuracy.append(knn.score(df_scaled_train_x, df_train_y))\n",
    "    test_accuracy.append(knn.score(df_scaled_test_x, df_test_y))\n",
    "\n",
    "df_accuracy_metric = pd.DataFrame()\n",
    "df_accuracy_metric[\"Neighbors\"] = para_n_neighbors\n",
    "df_accuracy_metric[\"Metric\"] = para_metric\n",
    "df_accuracy_metric[\"TrainAccuracy\"] = train_accuracy\n",
    "df_accuracy_metric[\"TestAccuracy\"] = test_accuracy\n",
    "df_accuracy_metric.round(3)\n",
    "\n",
    "df_accuracy_metric.groupby(\"Metric\").max().round(3)\n",
    "\n",
    "## 세 가지 방법의 최대 정확도 값을 구했을 때\n",
    "## manhattan 방법이 가장 높은 정확도를 기록했다는 것을 알 수 있다.\n",
    "\n",
    "df_accuracy_metric_pivot = df_accuracy_metric.pivot(index = \"Neighbors\", columns = \"Metric\",\n",
    "                                                     values=[\"TrainAccuracy\", \"TestAccuracy\"])\n",
    "level0 = df_accuracy_metric_pivot.columns.get_level_values(0)\n",
    "level1 = df_accuracy_metric_pivot.columns.get_level_values(1)\n",
    "df_accuracy_metric_pivot.columns = level0 + \"_\" + level1\n",
    "sns.lineplot(data = df_accuracy_metric_pivot)\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, weights = \"uniform\", metric = \"manhattan\")\n",
    "knn_model.fit(df_train_x, df_train_y)\n",
    "\n",
    "y_pred = knn_model.predict(df_test_x)\n",
    "\n",
    "print(\"train data accuracy: {0:.3f}\".format(knn_model.score(df_train_x, df_train_y)))\n",
    "print(\"test data accuracy: {0:.3f}\".format(knn_model.score(df_test_x, df_test_y)))\n",
    "print(\"Confusion matrix: \\n{}\".format(confusion_matrix(df_test_y, y_pred)))\n",
    "\n",
    "# 결론 :\n",
    "## 트레이닝 정확도는 94.3%\n",
    "## 테스트 정확도는 93.8%\n",
    "## 정분류율은 (30 + 90) / (30 + 90 + 1 + 7) * 100 = 93.75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
